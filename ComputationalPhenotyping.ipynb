{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Phenotyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will apply some unsupervised and supervised learning techniques in order to analyze phenotypic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the training data, as saved by [CellCognition](https://www.cellcognition-project.org). CellCognition saves the training set data as an `arff` file. In this file, we have all features for the annotated training samples and the class of each sample, saved in the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = arff.loadarff('./data/classifier/Classifier_2016_09_27/data/features.arff')\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next would like to load the class definitions. They are saved as a tab-delimited text file, containing:\n",
    "- the label of the class\n",
    "- the string representation of the class\n",
    "- the color that has been chosen during annotation\n",
    "\n",
    "We will therefore import these definitions by reading the csv-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('./data/classifier/Classifier_2016_09_27/class_definition.txt', 'r')\n",
    "reader = csv.reader(fp, delimiter='\\t')\n",
    "name_to_label = {}\n",
    "label_to_name = {}\n",
    "lut = {}\n",
    "for label, class_name, class_color in reader:\n",
    "    name_to_label[class_name] = int(label)\n",
    "    label_to_name[int(label)] = class_name\n",
    "    lut[int(label)] = class_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `arff`-file we have imported contains both the class labels and the features. Obviously, we need to separate in order to train classifiers, etc. We will therefore create a design matrix $X$ containing all columns of the data frame and an $y$ vector containing the class labels (where we take the labels from class_definition, rather than the string representation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([name_to_label[x.decode(\"utf-8\").strip('\\'')] for x in df['class']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('class', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now built our training set. We have 875 samples (annotated nuclei) and 235 features for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to check whether there are `Nan`'s in the data. `Nan` means \"not a number\"; this typically happens if a feature value was not defined for some reason (e.g. division by zero). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nans = np.isnan(X).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of Nan in the data: %i' % nb_nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no `Nan`s, so we can proceed. It must be noted that some learning algorithms can deal with `Nan`s, but often it is worth thinking about how to avoid them altogether, if possible. This is of course only possible if the data scientist can control the entire workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to check is whether there are features with zero-variance. They certainly contain absolutely no information (they are constant over all classes), and nothing can be learned from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[np.where(np.std(X)==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there is one feature that does not vary for the entire data set, we can therefore safely remove it. Again, for many Machine Learning algorithm, it does not hurt to keep the feature, but for some it can be detrimental. We therefore prefer to remove the corresponding column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['dyn_distance_radius_3', 'class'], axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next normalize the data. We clearly saw that the different features we are using ave very different ranges (some are between 0 and 1, others can reach very high values). Again, data normalization is not necessary for all machine learning algorithms. For some however, it is absolutely essential to apply data normalization before. Imagine for instance that you just calculate the Euclidean distance between data points $\\|x_i - x_j\\|_2$ (for instance for the purpose of clustering or visualization). In this case a feature with absolute values would completely dominate this metric. Here we use a standard normalization (zero mean and unity variance). If $x_j$ is one feature (column of $X$), we define $x^{norm}_j$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "x^{norm}_j = \\frac{x_j - \\mu(x_j)}{\\sigma_{x_j}}  \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnorm=(X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(Xnorm).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(Xnorm).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we work only on normalized features (i.e. we omit $norm$ in the formulas and set `X` to `Xnorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the data and unsupervised analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next turn to look at the data and show several ways of visually analyzing the data set. While this is often neglected in theoretical talks, looking at the data is usually an important part of a data scientist's work. It must be noted that in many real-world applications, in particular in biology, the definition of classes is not necessarily obvious and can undergo some modification during a project's lifetime. It is therefore important to get an idea about the trends in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of looking at the data is to visualize joint distributions of features via scatterplots of feature pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scatterplot(X, y, feature1, feature2, class_names):\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    ax.set_title('Scatter plot: %s vs. %s' % (feature2, feature1))\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_label = name_to_label[class_name]\n",
    "        class_color = lut[class_label]\n",
    "        ax.scatter(X[feature1][y==class_label],\n",
    "                   X[feature2][y==class_label],\n",
    "                   c=class_color, \n",
    "                   s=15)\n",
    "    ax.legend(class_names)\n",
    "    ax.grid()\n",
    "\n",
    "make_scatterplot(X, y, 'roisize', 'granu_open_volume_3', \n",
    "                 ['Interphase', 'Polylobed', 'Prometaphase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we plot a texture feature (morphological granulometries of size 3) and the number of pixels, i.e. one shape and one texture descriptor. We see that these features are quite discriminative between these classes. Not surprisingly, the pixelsize is capable of distinguishing between polylobed and interphase / prometaphase and the texture feature is capable of distinguishing prometaphase from interphase / polylobed. So we see that there is signal, but we also notice that the classes are far from being separable (by taking just these two features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, you might be tempted to reduce the number of features by removing features with little or no discriminative power and features that are strongly correlated to other features, as they -  so the idea - convey similar information. Let us take for instance 2 features describing second order statistics (`h1_2VAR` and `h4_1VAR`) and calculate the overall correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(X['h1_2VAR'], X['h4_2VAR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus see that we get a strong correlation of these two features (0.92) suggesting that we could safely remove one of them without loosing much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_scatterplot(X, y, 'h1_2VAR', 'h4_2VAR', ['Interphase', 'Prometaphase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue *Question: How do you interpret this graph? What can be said about the importance of the two features?*>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> *How do you interpret this plot? Are these features useful?*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next way of visualizing the data is to find a low dimensional reprentation. There are many methods in order to achieve this goal. The two most popular methods are Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE). In the following, we will apply these methods on our training data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA projects the feature data onto a space where all components are orthogonal to each other and have maximal variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA (training set)')\n",
    "\n",
    "class_names = name_to_label.keys()\n",
    "for class_name in class_names:\n",
    "    class_label = name_to_label[class_name]\n",
    "    class_color = lut[class_label]\n",
    "    ax.scatter(principalComponents[y==class_label][:,0],\n",
    "               principalComponents[y==class_label][:,1],\n",
    "               c=class_color, \n",
    "               s=15)\n",
    "ax.legend(class_names)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> *Observations? Which classes would you expect to be difficult to classify correctly? How do you interpret the different region sizes occupied by different classes?*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we turn to a newer projection technique which is widely used in Machine Learning, but also in Computational Biology (in particular for sequencing data): t-SNE. In this method, one considers the probabilities $p_{ij}$ of two points $x_i$ and $x_j$ to be considered as neighbors. t-SNE aims at finding a low-dimensional feature space, where the distribution of the corresponding $q_{ij}$ are as close as possible to $p_{ij}$ in the sense of KL-divergence:\n",
    "\\begin{equation}\n",
    "KL(P\\|Q) = \\sum_{i\\not= j}p_{ij}\\log\\frac{p_{ij}}{q_{ij}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('tSNE-component 1')\n",
    "ax.set_ylabel('tSNE-component 2')\n",
    "ax.set_title('tSNE')\n",
    "\n",
    "class_names = name_to_label.keys()\n",
    "for class_name in class_names:\n",
    "    class_label = name_to_label[class_name]\n",
    "    class_color = lut[class_label]\n",
    "    ax.scatter(X_embedded[y==class_label][:,0],\n",
    "               X_embedded[y==class_label][:,1],\n",
    "               c=class_color, \n",
    "               s=15)\n",
    "ax.legend(class_names)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> *How does this compare to the results obtained by PCA?*</font>\n",
    "\n",
    "<font color=red> *Test different values of perplexity.*</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorvec = [lut[label] for label in y]\n",
    "cmap = sns.diverging_palette(10, 220, sep=80, n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(X, method='single', metric='euclidean', \n",
    "                   row_colors=colorvec, robust=True, cmap=cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> *Try different agglomeration functions: single, complete, average, centroid, Ward (check in the sklearn documentation, what they mean and conclude, which one you would like to use).* </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we observe that some classes can be well distinguished, others are clustered together. When interpreting such a plot, it is essential to also look at the dendrogram. Actually, the true clustering information is represented by the dendrogram and not by the order of the rows. Indeed, I could flip the cluster order at any merging point of the dendrogram without changing the clustering result provided by the hierarchical clustering. In the above example with `method=\"ward\"` and `metric=\"euclidean\"`, this concerns for instance the distinction between the lower part of the anaphase/metaphase block and the apoptosis block next to it: in reality, these cells are (wrongly) clustered together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train classifiers on our training set. Here, we train Random Forest Classifiers (RF) and Support Vector Machines (SVM) in order to recognize nuclear morphologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are ensemble methods, where decision trees are trained on random subsets of the training data, where each split is performed on one feature. This feature is chosen from a random subset of the features in order to optimize the purity of the resulting sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define the hyperparameters. In practice, on our kind of data, the most important parameter to be set is the number of trees in the forest. In general, the higher this number the better it is, but the resulting classifier will be slower also. Importantly, this parameter does not only affect training time, but also prediction time. For us however, this is not critical, and we set this parameter to 200. The second parameter is the `oob_score` which indicates whether to calculate the out-of-bag error. This is an error wich is only calculated on those samples that were left out for training the tree. Using the out-of-bag error allows us to avoid time-consuming cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, oob_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall accuracy: %.2f' % rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now calculate the confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result = rf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_result = np.argmax(prediction_result, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_labels = np.array(sorted(label_to_name.keys()))\n",
    "cm_rf = confusion_matrix(y, class_labels[classification_result])\n",
    "\n",
    "print(', '.join([label_to_name[l] for l in class_labels]))\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, training is formulated as a convex optimization problem under constraints:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\min_{w,\\xi} & & \\|w\\|^2 + C \\sum_{i=1}^{N}\\xi_i\\\\\n",
    "\\mbox{subject to} & & y_i(w^Tx_i + b) \\geq 1 - \\xi_i \\quad i = 1, \\ldots, N \\\\\n",
    "& & \\xi_i \\geq 0 \\quad i = 1, \\ldots, N\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The only parameter to be fixed is $C$, which controls the compromise between classifcation errors and regularization (the higher $C$ is chosen, the more weight is given to correct classification of training samples). There is no way of guessing $C$ and so we need to test different values of $C$ and choose the value that leads to the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(C=0.01, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to evaluate the performance of the classifier by cross-validation: the training error is not equal to the test error. We are going to show this. First, we calculate the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy measured on the training set: %.2f' % np.mean(clf.score(X,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(clf, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy measured by 10-fold cross validation: %.2f' % np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see that the performance on the training set is better than on the test set. Indeed, evaluating the performance on the training set is a typical mistake in machine learning, as this always gives overoptimistic results. While in this case, this was easy to avoid, it is sometimes more challenging to know whether our performance evaluation is correct in that it does not intermingle training and test set.\n",
    "\n",
    "Here, we obtain a very similar result to what we obtained from the RF classifier, even though we use a linear kernel and did not yet optimize the parameter $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cross_val_predict(clf, X, y)\n",
    "\n",
    "cm_rf = confusion_matrix(y, res)\n",
    "\n",
    "print(', '.join([label_to_name[l] for l in class_labels]))\n",
    "print(cm_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':[0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100, 500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(clf, parameters, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we see that for this case, optimization of the parameter $C$ only marginally changes the accuracy. This is unusual, but it can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red> *Train a classifier on the principal components. Plot the accuracy depending on the number of the principal components used. For 4 components, test the effect of the parameter C.* </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
